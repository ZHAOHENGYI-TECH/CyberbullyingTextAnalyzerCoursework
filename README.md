# CyberbullyingTextAnalyzerCoursework
nottingham programming group project 
Group Name:B&G
Group Members:
ZHAOHENGYI:20792131
LILINGYU:20791542
WANGJINGBO:20795326
The analyzer can detect different levels of toxic language:
- Mild toxicity: words like silly, boring, dumb
- Moderate toxicity: words like stupid, annoying, hate  
- Severe toxicity: words like crap, hell, and offensive phrases


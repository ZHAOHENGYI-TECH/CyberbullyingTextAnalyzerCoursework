# CyberbullyingTextAnalyzerCoursework
nottingham programming group project 

The analyzer can detect different levels of toxic language:
- Mild toxicity: words like silly, boring, dumb
- Moderate toxicity: words like stupid, annoying, hate  
- Severe toxicity: words like crap, hell, and offensive phrases

